from urllib import parseimport MySQLdbimport requestsimport scrapyfrom flask import jsonfrom scrapy import Selector, Requestfrom tools.getFilterName import getInternsAllCityDataValfrom RecruitSpider.items import InternItemLoader, InternCompanyItem, InternPositionItemclass internsSpider(scrapy.Spider):    name = 'interns'    #  拿到所有城市的编码    city_list = []    cities = getInternsAllCityDataVal()    for city in cities:        city_list.append(city)    start_urls = ['https://www.shixiseng.com/interns/c-code_st-intern_?k=&p=1'.replace('code', x) for x in city_list]    allowed_domains = ['wwww.shixiseng.com']    custom_settings = {        'ITEM_PIPELINES': {            'RecruitSpider.pipelines.InternsSpiderPipeline': 300,        },    }    headers = {        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8",        "Accept-Encoding": "gzip, deflate, br",        "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",        "Cache - Control": "max - age = 0",        'Connection': 'keep-alive',        'Cookie': 'SXS_XSESSION_ID="2|1:0|10:1521077606|15:SXS_XSESSION_ID|48:MTg5NmVhOTEtZmU0NC00NTk0LWFmYzEtMjFlNWZjMGI1YmZi|324103c8d11bf3882c2ff864ed5d0a8cb7e42558777d485cf421e538a282bcc1";'                  'SXS_XSESSION_ID_EXP="2|1:0|10:1521077606|19:SXS_XSESSION_ID_EXP|16:MTUyMTE2NDAwNg==|e41d3e9cfcdd4595787d72d06ad1976c4e630ecbd65e4635596ec3ca06ba5e3d"; '                  '__jsluid=92a7e5bb1052c0cafbfbf75563c33630;'                  'uuid=a77d8e60-f03d-8ee1-8ae6-4db1bf48075d;'                  'Hm_lvt_03465902f492a43ee3eb3543d81eba55=1521077807;'                  'gr_user_id=b68acace-2414-42f7-a461-ab3eba054727;'                  'MEIQIA_EXTRA_TRACK_ID=0z1EoLDhsQNJ1PnuGntqc7ubTLM;'                  'search=; gr_session_id_96145fbb44e87b47=65c0a81a-403e-4c05-827b-b4195a0bd4cc;'                  'gr_cs1_65c0a81a-403e-4c05-827b-b4195a0bd4cc=user_id%3Anull;'                  'uid1=27451cff-000d-444f-95fc-0a53f449cea7; uid2=2a379ca3-706f-fbf3-ad13-ac6686173775;'                  'SXS_VISIT_XSESSION_ID_V3.0="2|1:0|10:1521093042|26:SXS_VISIT_XSESSION_ID_V3.0|48:MTg5NmVhOTEtZmU0NC00NTk0LWFmYzEtMjFlNWZjMGI1YmZi|51c1cfdb22809d332e4c9d549aeb98231d77189fe15bd47d3570df461e71c068";'                  'SXS_VISIT_XSESSION_ID_V3.0_EXP="2|1:0|10:1521093042|30:SXS_VISIT_XSESSION_ID_V3.0_EXP|16:MTUyMzY4NTA0Mg==|85b9f4c0e255d0acc3022921f05cc33eb5545a2c8633ef6616521e5a2f9b4171";'                  'Hm_lpvt_03465902f492a43ee3eb3543d81eba55=1521093047',        'Host': 'www.shixiseng.com',        'Referer': 'https://www.shixiseng.com/',        'Upgrade-Insecure-Requests': 1,        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36',    }    url_list = []    num = 0    def parse(self, response):        # 初始化选择器和item        selector = Selector(response)        # 如果超出最大页数, 就不执行抓取        page = selector.xpath("//div[@id='pagebar']/ul/li/@class").extract()[-1]        # print(page)        if page != 'disabled':            # 爬取一页的职业信息, 返回一个list            positionName =  response.xpath('//div[@class="position"]/div[@class="position-center clearfix"]/div[@class="position-list-box"]/'                                           'ul[@class="position-list"]/li[@class="font"]/div[@class="info1"]/div[@class="name-box clearfix"]/a[@class="name"]/text()').extract()            # 爬取指向每个职位详情页的链接            positionUrl = response.xpath('//div[@class="position"]/div[@class="position-center clearfix"]/div[@class="position-list-box"]/ul[@class="position-list"]/li[@class="font"]/div[@class="info1"]'                                         '/div[@class="name-box clearfix"]/a[@class="name"]/@href').extract()            # 爬取公司信息的链接            companyUrl = response.xpath(                '//div[@class="position"]/div[@class="position-center clearfix"]/div[@class="position-list-box"]/ul[@class="position-list"]/li[@class="font"]/div[@class="info1"]'                '/div[@class="company-box"]/a[@class="company"]/@href').extract()            item_list = []      #职位列表的list            inn_uuid_list = []  #职位的uuid            com_uuid_list =[]   #公司的uuid            for i in range(len(positionName)):                inn_uuid =  positionUrl[i].replace("/intern/", "")                com_uuid = companyUrl[i].replace("/com/", "")                inn_uuid_list.append(inn_uuid)                com_uuid_list.append(com_uuid)                item_list.append(positionName[i])            for i in range(len(item_list)):                # 拼接的api                inn_url = "https://wap.shixiseng.com/app/intern/info?uuid="+ inn_uuid_list[i]                com_url = "https://wap.shixiseng.com/app/company/info?uuid="+ com_uuid_list[i]                yield Request(url=inn_url , callback=self.parse_datial, dont_filter=True)                yield Request(url=com_url , callback=self.parse_datial, dont_filter=True)                self.num += 1                self.url_list.append(inn_url)                self.url_list.append(com_url)            # while self.num < len(self.url_list):            #     yield Request(url=self.url_list[self.num], callback=self.parse_datial, dont_filter=True)            #     self.num += 1            # 拼接好下一页的url并爬取下一页            if "p=" in response.url[-3:]:                next_page_index = int(response.url[-1:]) + 1                next_page = response.url[:-1] + str(next_page_index)            elif "=" in response.url[-3:]:                next_page_index = int(response.url[-2:]) + 1                next_page = response.url[:-2] + str(next_page_index)            else:                next_page_index = int(response.url[-3:]) + 1                next_page = response.url[:-3] + str(next_page_index)            yield Request(url=next_page, callback=self.parse, dont_filter=True)        # for i in range(self.num, len(self.url_list)):    def parse_datial(self, response):        item_loader = InternItemLoader(item=InternCompanyItem(), response=response)        item_loader2 = InternItemLoader(item=InternPositionItem(), response=response)        url = response.url        html = requests.get(url).content  # 获取首页的html        msg_dict = json.loads(html)        dict =msg_dict['msg']        info = dict['info'].replace('\s', '')        info = info.replace("'", "`")        description = dict['description'].replace("'", "`")        # 职位信息 25 条, 公司信息21条        if len(dict) == 25:            # print("=========================职位信息====================================")            item_loader2.add_value("positionName", dict['iname'].replace("'", "`"))            item_loader2.add_value("month", dict['month'])            item_loader2.add_value("maxsal", dict['maxsal'])            item_loader2.add_value("com_logo", dict['logo'])            item_loader2.add_value("minsal", dict['minsal'])            item_loader2.add_value("city", dict['city'])            item_loader2.add_value("com_scale", dict['scale'] if dict['scale'] else 'NULL')            item_loader2.add_value("reslan", dict['reslan'] if dict['reslan'] else 'NULL')            item_loader2.add_value("attraction", dict['attraction'].replace("'", "`"))            item_loader2.add_value("ftype", dict['ftype'])            item_loader2.add_value("collected", dict['collected'])            item_loader2.add_value("cuuid", dict['cuuid'])            item_loader2.add_value("degree", dict['degree'])            item_loader2.add_value("delivered", dict['delivered'])            item_loader2.add_value("chance", dict['chance'])            item_loader2.add_value("work_address", dict['address'] if dict['address'] else 'NULL')            item_loader2.add_value("endtime", dict['endtime'])            item_loader2.add_value("day", dict['day'])            item_loader2.add_value("work_info", info)            item_loader2.add_value("positionUrl", dict['url'])            item_loader2.add_value("com_industry", dict['industry'] if dict['industry'] else 'NULL')            item_loader2.add_value("refresh", dict['refresh'])            item_loader2.add_value("com_name", dict['cname'])            item_loader2.add_value("invited", dict['invited'])            item_loader2.add_value("overdue", dict['overdue'])            intern_item2 = item_loader2.load_item()            yield intern_item2        else:            # print("=========================公司信息====================================")            item_loader.add_value("company_info", info if info else 'NULL')            item_loader.add_value("reg_num", dict['reg_num'])            item_loader.add_value("scale", dict['scale'] if dict['scale'] else 'NULL')            item_loader.add_value("company_name", dict['name'])            item_loader.add_value("url", dict['url'])            item_loader.add_value("industry", dict['industry'] if dict['industry'] else 'NULL')            item_loader.add_value("pranum", dict['pranum'])            item_loader.add_value("reg_name", dict['reg_name'])            item_loader.add_value("wurl", dict['wurl'] if  dict['wurl'] else 'NULL')            item_loader.add_value("is_collect", dict['is_collect'])            item_loader.add_value("cname", dict['cname'])            item_loader.add_value("vote_url", dict['vote_url'] if  dict['vote_url'] else 'NULL')            item_loader.add_value("address", dict['address'] if dict['address'] else 'NULL')            item_loader.add_value("com_url", dict['com_url'])            item_loader.add_value("logo", dict['logo'])            item_loader.add_value("com_type", dict['com_type'])            item_loader.add_value("reg_capi", dict['reg_capi'])            item_loader.add_value("start_time", dict['start_time'])            item_loader.add_value("description", description if description else 'NULL')            intern_item = item_loader.load_item()            yield intern_item